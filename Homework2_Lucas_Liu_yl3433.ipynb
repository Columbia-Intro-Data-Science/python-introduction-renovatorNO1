{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "In this homework, you'll be required to load in a dataset which has about 500 features. By using\n",
    "Lasso ($L^1$) regression, we'll find the optimal constraint on the $L^1$ norm which gives us the best\n",
    "$R^2$. Then we'll plot the results.\n",
    "\n",
    "Recall we minimize the following on ** training data: $(x_i,y_i)$**\n",
    "\n",
    "$$\\min_{\\beta} \\frac{1}{N} \\sum_{i=1}^N (y_i - \\beta \\cdot x_i)^2 + \\lambda \\|\\beta \\|_{L^1}.$$\n",
    "\n",
    "\n",
    "Denoting $\\beta_{\\lambda}$ as the minimum of the above, we then choose $\\lambda$ to maximize $R^2$ on **testing data: $(x_j,y_j)$**\n",
    "\n",
    "$$ \\max_{\\lambda} 1 - \\frac{\\sum_{j} (y_j - \\beta_{\\lambda} \\cdot x_j)^2}{\\sum_j (y_j - \\bar y)^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load in hw2data.csv from ../data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy.random as nprnd\n",
    "import random\n",
    "import json\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/hw2data.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Set to be the y variable in the dataframe from a and X to be the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['y']\n",
    "X = df.drop('y', 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) As shown in the Booking.com example, using Lasso regression, find the regularization strength\n",
    "which optimizes the $R^2$. \n",
    "\n",
    "**Hint:** Take a range of alpha from `np.logspace(-8,-3,1000)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alphas = np.logspace(-5,0,1000)\n",
    "scores = []   \n",
    "\n",
    "#best_alpha = alphas[best_alpha_index]\n",
    "train_errors=[]\n",
    "test_errors=[]\n",
    "\n",
    "for alpha in alphas:\n",
    "    regr = Lasso(alpha=alpha)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_train, y_train)\n",
    "    train_errors.append(regr.score(X_train, y_train))\n",
    "    test_errors.append(regr.score(X_test,y_test))\n",
    "\n",
    "best_alpha_index = np.argmax(test_errors)\n",
    "alpha_optim= alphas[best_alpha_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Plot the training perforamnce versus the testing performance, and observe whree the test performance is\n",
    "maximized. I've written an outline of the code you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.semilogx(alphas, train_errors, label='Train')\n",
    "plt.semilogx(alphas, test_errors, label='Test')\n",
    "plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left')\n",
    "plt.ylim([0, 1.2])\n",
    "plt.xlabel('Regularization parameter')\n",
    "plt.ylabel('Performance')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Plot the top coefficients based on this optimal paramter. Why do you think so many are zero? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='../images/lesso.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure is contour plot of Lasso regression objective function. The elliptical contour plot in the figure represents sum of squared errors. The diamond shape in the middle indicates the constraint region. The optimal point is a point which is common point between ellipse and circle as well as gives a minimum value for the above function. There is a high probability that optimum point falls in the corner point of diamond region, which renders one of the variable(coefficient) zero.\n",
    "\n",
    "To put it another way:\n",
    "$$\\min_{\\beta} \\frac{1}{N} \\sum_{i=1}^N (y_i - \\beta \\cdot x_i)^2 + \\lambda \\|\\beta \\|_{L^1}.$$\n",
    "\n",
    "In the above equation, if $\\lambda$ is sufficiently large, some of the coefficients are driven to zero, leading to a sparse model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "regr = Lasso(alpha=alpha_optim )\n",
    "regr.fit(X_train,y_train)\n",
    "regr.coef_\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "df_coeffs = pd.DataFrame({'coeffs':regr.coef_, 'name':X.columns.values})\n",
    "df_coeffs=df_coeffs.sort(['coeffs'])\n",
    "df_coeffs[::-1].plot(x='name',y='coeffs',kind='bar', figsize=(14,14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Compute the $R^2$ with the optimal coefficient found above on 5 folds using cross_val_score and plot the\n",
    "results. Does the model work well on all random subsets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "regr = Lasso(alpha=alpha_optim)\n",
    "scores = cross_val_score(regr, X, y, cv=5)\n",
    "\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('R^2')\n",
    "plt.title('Performance on 5 folds with Original Subsets lambda=' + str(alpha))\n",
    "plt.bar(range(1,6),scores)\n",
    "plt.show()\n",
    "R_average = np.mean(scores)\n",
    "print(\"Normal R^2 value is \\n\", R_average)\n",
    "\n",
    "#Randomized\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "X_random, y_random = shuffle(X, y, random_state=random_state)\n",
    "regr = Lasso(alpha=alpha_optim)\n",
    "scores = cross_val_score(regr, X_random, y_random, cv=5)\n",
    "\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('R^2')\n",
    "plt.title('Performance on 5 folds with Randomized Subsets with lambda=' + str(alpha))\n",
    "plt.bar(range(1,6),scores)\n",
    "plt.show()\n",
    "R_average = np.mean(scores)\n",
    "print(\"Randomized R^2 value is \\n\", R_average)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model works well with random subsets because the both R^2 of random subsets and original subsets agree with one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Repeat e) but using cross validation. Use error bars on the features which are the standard deviation of the \n",
    "coefficiens obtained above. For this problem I\"ll walk you through the code. You just need to apply your optimal\n",
    "$\\alpha$ found above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import preprocessing\n",
    "def run_cv_coeffs(X,y,clf_class,**kwargs):\n",
    "    # Construct a kfolds object\n",
    "    kf = KFold(len(y),n_folds=5,shuffle=True)\n",
    "    y_pred = y.copy()\n",
    "    coeffs=[]\n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        # Initialize a classifier with key word arguments\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "        coeffs.append(clf.coef_)\n",
    "    return coeffs\n",
    "\n",
    "alpha_best = alpha_optim\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_scaled = X.as_matrix().astype(np.float)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "coeffs=run_cv_coeffs(X_scaled,np.array(y),Lasso,alpha=alpha_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_coeffs(coeffs):\n",
    "    coeffs_avgd = [(coeffs[0][i] + coeffs[1][i] + coeffs[2][i] + coeffs[3][i] + coeffs[4][i])/5 for i in range(0,len(X.columns))]\n",
    "    coeffs_std = [np.std([coeffs[0][i],coeffs[1][i],coeffs[2][i],coeffs[3][i],coeffs[4][i]]) for i in range(0,len(X.columns))]\n",
    "    return coeffs_avgd, coeffs_std\n",
    "coeffs_avg,coeffs_std=get_coeffs(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfCoeffs = pd.DataFrame({'type':X.columns.values, 'coef':coeffs_avg, 'std':coeffs_std})\n",
    "dfCoeffs = dfCoeffs[(dfCoeffs['coef']>1) |(dfCoeffs['coef']<-1) ]\n",
    "plt.figure(figsize=(15,15))\n",
    "dfCoeffs_sorted = dfCoeffs.sort(['coef'])[::-1]\n",
    "yerr_vals = dfCoeffs_sorted['std'].values\n",
    "dfCoeffs_sorted.plot(x='type',y='coef',kind='bar',yerr=yerr_vals,figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
